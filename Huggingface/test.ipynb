{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-26T10:15:38.225288Z",
     "start_time": "2024-08-26T10:15:31.195471Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T10:30:38.557845Z",
     "start_time": "2024-08-26T10:30:38.495476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"./data/labels\",split=\"train\")\n",
    "dogs = dataset.train_test_split(test_size=0.2)"
   ],
   "id": "e39ed4ccac54e6ef",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T10:30:40.264916Z",
     "start_time": "2024-08-26T10:30:40.240836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "breed = \"data/breed.csv\"\n",
    "filename = \"data/labels.csv\"\n",
    "labelsNames = [\"id\", \"breed\"]\n",
    "csvlabels = read_csv(filename, names=labelsNames)\n",
    "\n",
    "breedNames = read_csv(breed)[\"breed\"].tolist()[1:]\n",
    "print(csvlabels.shape)\n",
    "# print(len(breedNames))"
   ],
   "id": "8d317caedf44454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12001, 2)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T10:30:48.080041Z",
     "start_time": "2024-08-26T10:30:48.058393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = csvlabels[\"breed\"]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "labels"
   ],
   "id": "b2fdcb3e95ff2c62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      breed\n",
       "1                  chihuahua\n",
       "2                  chihuahua\n",
       "3                  chihuahua\n",
       "4                  chihuahua\n",
       "                ...         \n",
       "11996    african_hunting_dog\n",
       "11997    african_hunting_dog\n",
       "11998    african_hunting_dog\n",
       "11999    african_hunting_dog\n",
       "12000    african_hunting_dog\n",
       "Name: breed, Length: 12001, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T09:36:18.931236Z",
     "start_time": "2024-08-26T09:36:16.855362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ],
   "id": "715d899478edda23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f00c74135384f3abb1cdd09ccc77237"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "889bd2ded5b44627920107406e46ad1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T09:37:02.155267Z",
     "start_time": "2024-08-26T09:37:02.145593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ],
   "id": "a1d6f842f6a2b202",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T09:37:03.025291Z",
     "start_time": "2024-08-26T09:37:03.007867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ],
   "id": "79fa467971418ecc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T09:58:53.035251Z",
     "start_time": "2024-08-26T09:58:53.005155Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "315f766eebc5f75e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object HfApi.list_datasets at 0x0000027E636C09E0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ],
   "id": "746f6be2c4f004b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ],
   "id": "fec5e96380f12e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    label2id=train_loader.dataset.data_label,\n",
    "    id2label={v: k for k, v in train_loader.dataset.data_label.items()},\n",
    ")"
   ],
   "id": "94ab9687b96b14df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
